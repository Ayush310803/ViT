{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2319455,
          "sourceType": "datasetVersion",
          "datasetId": 1392101
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush310803/ViT/blob/main/unet_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content\"\n",
        "!kaggle datasets download -d rajkumarl/people-clothing-segmentation\n",
        "\n",
        "dataset_zip = \"/content/people-clothing-segmentation.zip\"\n",
        "extract_path = \"/content/dataset\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Dataset extracted successfully.\")\n",
        "!ls /content/dataset\n",
        "\n",
        "image_dir = \"/content/dataset/jpeg_images/IMAGES\"\n",
        "mask_dir = \"/content/dataset/jpeg_masks/MASKS\"\n",
        "label_path = \"/content/dataset/labels.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSKt5nxcNu1t",
        "outputId": "050fa125-f986-47ff-8710-3193073f672e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/rajkumarl/people-clothing-segmentation\n",
            "License(s): CC0-1.0\n",
            "people-clothing-segmentation.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "✅ Dataset extracted successfully.\n",
            " jpeg_images   jpeg_masks  'labels (1).csv'   labels.csv   png_images   png_masks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms.functional as tfunc\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:13.062225Z",
          "iopub.execute_input": "2025-01-10T05:20:13.062529Z",
          "iopub.status.idle": "2025-01-10T05:20:22.845112Z",
          "shell.execute_reply.started": "2025-01-10T05:20:13.062508Z",
          "shell.execute_reply": "2025-01-10T05:20:22.844229Z"
        },
        "scrolled": true,
        "id": "h_kRqZ9hNfLf"
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConvolution(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.first(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.second(x)\n",
        "        return self.act2(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:31.699517Z",
          "iopub.execute_input": "2025-01-10T05:20:31.699931Z",
          "iopub.status.idle": "2025-01-10T05:20:31.704702Z",
          "shell.execute_reply.started": "2025-01-10T05:20:31.699897Z",
          "shell.execute_reply": "2025-01-10T05:20:31.703967Z"
        },
        "id": "U5PmBImGNfLh"
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.pool(x)\n",
        "\n",
        "# Define the UpSample class\n",
        "\n",
        "\n",
        "# Define the CropAndConcat class\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:32.803454Z",
          "iopub.execute_input": "2025-01-10T05:20:32.803735Z",
          "iopub.status.idle": "2025-01-10T05:20:32.807633Z",
          "shell.execute_reply.started": "2025-01-10T05:20:32.803714Z",
          "shell.execute_reply": "2025-01-10T05:20:32.806852Z"
        },
        "id": "aIVGNij0NfLh"
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.up(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:33.068693Z",
          "iopub.execute_input": "2025-01-10T05:20:33.068904Z",
          "iopub.status.idle": "2025-01-10T05:20:33.072909Z",
          "shell.execute_reply.started": "2025-01-10T05:20:33.068886Z",
          "shell.execute_reply": "2025-01-10T05:20:33.072174Z"
        },
        "id": "EXFWxV5cNfLh"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "class CropAndConcat(nn.Module):\n",
        "    def forward(self, x: torch.Tensor, x_contract: torch.Tensor):\n",
        "        x_contract = tfunc.center_crop(x_contract, [x.shape[2], x.shape[3]])\n",
        "        x = torch.cat([x, x_contract], dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:35.187165Z",
          "iopub.execute_input": "2025-01-10T05:20:35.187513Z",
          "iopub.status.idle": "2025-01-10T05:20:35.191994Z",
          "shell.execute_reply.started": "2025-01-10T05:20:35.187486Z",
          "shell.execute_reply": "2025-01-10T05:20:35.191068Z"
        },
        "id": "TEJsueh5NfLi"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
        "                                        [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n",
        "        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
        "        self.mid_conv = DoubleConvolution(512, 1024)\n",
        "        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
        "        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
        "                                      [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
        "        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        pass_thru = []\n",
        "        for i in range(len(self.down_conv)):\n",
        "            x = self.down_conv[i](x)\n",
        "            pass_thru.append(x)\n",
        "            x = self.down_sample[i](x)\n",
        "        x = self.mid_conv(x)\n",
        "        for i in range(len(self.up_conv)):\n",
        "            x = self.up_sample[i](x)\n",
        "            x = self.concat[i](x, pass_thru.pop())\n",
        "            x = self.up_conv[i](x)\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:35.373986Z",
          "iopub.execute_input": "2025-01-10T05:20:35.374186Z",
          "iopub.status.idle": "2025-01-10T05:20:35.381282Z",
          "shell.execute_reply.started": "2025-01-10T05:20:35.374169Z",
          "shell.execute_reply": "2025-01-10T05:20:35.380578Z"
        },
        "id": "tKRSW7cdNfLi"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "class ClothingSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:40.951307Z",
          "iopub.execute_input": "2025-01-10T05:20:40.951588Z",
          "iopub.status.idle": "2025-01-10T05:20:40.957009Z",
          "shell.execute_reply.started": "2025-01-10T05:20:40.951568Z",
          "shell.execute_reply": "2025-01-10T05:20:40.956203Z"
        },
        "id": "rmX1P7k1NfLi"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = ClothingSegmentationDataset(image_dir, mask_dir, transform=transform)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:42.931491Z",
          "iopub.execute_input": "2025-01-10T05:20:42.931850Z",
          "iopub.status.idle": "2025-01-10T05:20:42.935541Z",
          "shell.execute_reply.started": "2025-01-10T05:20:42.931821Z",
          "shell.execute_reply": "2025-01-10T05:20:42.934759Z"
        },
        "id": "Rts8B7KmNfLi"
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = int(0.1 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "xtrain, xval, xtest = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(xtrain, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(xval, batch_size=16, shuffle=False)\n",
        "\n",
        "label_path = \"/content/dataset/labels.csv\"\n",
        "labels = pd.read_csv(label_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:44.876376Z",
          "iopub.execute_input": "2025-01-10T05:20:44.876662Z",
          "iopub.status.idle": "2025-01-10T05:20:44.880415Z",
          "shell.execute_reply.started": "2025-01-10T05:20:44.876642Z",
          "shell.execute_reply": "2025-01-10T05:20:44.879412Z"
        },
        "id": "mGSVX5P_NfLi"
      },
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(3, 59)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:45.848798Z",
          "iopub.execute_input": "2025-01-10T05:20:45.849085Z",
          "iopub.status.idle": "2025-01-10T05:20:45.852854Z",
          "shell.execute_reply.started": "2025-01-10T05:20:45.849061Z",
          "shell.execute_reply": "2025-01-10T05:20:45.852023Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djwWImSENfLj",
        "outputId": "07fa7622-6ca6-490d-a3b0-0bcaf7401fec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (down_sample): ModuleList(\n",
              "    (0-3): 4 x DownSample(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (mid_conv): DoubleConvolution(\n",
              "    (first): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (second): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "  )\n",
              "  (up_sample): ModuleList(\n",
              "    (0): UpSample(\n",
              "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): UpSample(\n",
              "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (2): UpSample(\n",
              "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): UpSample(\n",
              "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (up_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (concat): ModuleList(\n",
              "    (0-3): 4 x CropAndConcat()\n",
              "  )\n",
              "  (final_conv): Conv2d(64, 59, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        masks = masks.squeeze(1)\n",
        "        masks = masks.long()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}\")\n",
        "\n",
        "    if (epoch+1)%5 == 0:\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            masks = masks.squeeze(1)\n",
        "            masks = masks.long()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "            scheduler.step(val_loss)\n",
        "        print(f\"Validation Loss: {val_loss/len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cta9wIRrRFdk",
        "outputId": "cf53ebb1-e8da-4e45-8a15-f9717033256a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/25, Loss: 3.3288028228282927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(xtest, batch_size=16, shuffle=False)\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "for images, masks in test_loader:\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "    masks = masks.squeeze(1)\n",
        "    masks = masks.long()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, masks)\n",
        "    test_loss += loss.item()\n",
        "print(f\"Test Loss: {test_loss/len(test_loader)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:46.463484Z",
          "iopub.execute_input": "2025-01-10T05:20:46.463739Z",
          "iopub.status.idle": "2025-01-10T05:20:46.470912Z",
          "shell.execute_reply.started": "2025-01-10T05:20:46.463721Z",
          "shell.execute_reply": "2025-01-10T05:20:46.470344Z"
        },
        "id": "N1-r9P5HNfLj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_iter = iter(val_loader)\n",
        "images, masks = next(val_iter)\n",
        "img_idx = 0\n",
        "image = images[img_idx]\n",
        "mask = masks[img_idx]\n",
        "output = model(image.to(device))\n",
        "image = F.to_pil_image(output)\n",
        "mask = F.to_pil_image(mask)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(image)\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].imshow(mask, cmap=\"gray\")\n",
        "ax[1].set_title(\"Mask\")\n",
        "ax[1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T05:20:48.015558Z",
          "iopub.execute_input": "2025-01-10T05:20:48.015826Z",
          "iopub.status.idle": "2025-01-10T05:20:48.038771Z",
          "shell.execute_reply.started": "2025-01-10T05:20:48.015806Z",
          "shell.execute_reply": "2025-01-10T05:20:48.037976Z"
        },
        "id": "jxOpz6xsNfLj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}